@article{Hartgerink2015,
   author = {C H J Hartgerink and R C M van Aert and M B Nuijten and J M Wicherts and M A L M van Assen},
   doi = {10.7717/peerj.1935},
   journal = {PeerJ},
   title = {Distributions of p-values smaller than .05 in psychology: What is going on?},
   volume = {4:e1935},
   url = {https://doi.org/10.7717/peerj.1935 https://peerj.com/articles/1935.pdf},
   year = {2015},
}
@article{VanAert2019a,
   author = {R C M van Aert and M A L M van Assen},
   doi = {10.31222/osf.io/zqjr9},
   title = {Correcting for publication bias in a meta-analysis with the p-uniform* method. Manuscript submitted for publication},
   url = {https://osf.io/preprints/metaarxiv/zqjr9/ https://osf.io/preprints/metaarxiv/zqjr9/download https://osf.io/ebq6m/},
   year = {2021},
}
@book{VanAert2018,
   author = {R C M van Aert},
   doi = {10.31222/osf.io/eqhjd},
   isbn = {9789463323673 9463323678},
   publisher = {GVO drukkers & vormgevers},
   title = {Meta-analysis: Shortcomings and potential},
   url = {https://osf.io/preprints/metaarxiv/eqhjd https://osf.io/preprints/metaarxiv/eqhjd/download},
   year = {2018},
}
@article{VanAssen2015,
   author = {M A L M van Assen and R C M van Aert and J M Wicherts},
   doi = {10.1037/met0000025},
   issue = {3},
   journal = {Psychological Methods},
   pages = {293-309},
   title = {Meta-analysis using effect size distributions of only statistically significant studies},
   volume = {20},
   url = {http://dx.doi.org/10.1037/met0000025 NA},
   year = {2015},
}
@article{Wicherts2016,
   abstract = {The designing, collecting, analyzing, and reporting of psychological studies entail many choices that are often arbitrary. The opportunistic use of these so-called researcher degrees of freedom aimed at obtaining statistically significant results is problematic because it enhances the chances of false positive results and may inflate effect size estimates. In this review article, we present an extensive list of 34 degrees of freedom that researchers have in formulating hypotheses, and in designing, running, analyzing, and reporting of psychological research. The list can be used in research methods education, and as a checklist to assess the quality of preregistrations and to determine the potential for bias due to (arbitrary) choices in unregistered studies.},
   author = {J M Wicherts and C L S Veldkamp and H E M Augusteijn and M Bakker and R C M van Aert and M A L M van Assen},
   doi = {10.3389/fpsyg.2016.01832},
   isbn = {1664-1078},
   issue = {1832},
   journal = {Frontiers in Psychology},
   keywords = {questionable research practices,Experimental desig},
   title = {Degrees of freedom in planning, running, analyzing, and reporting psychological studies: A checklist to avoid p-hacking},
   volume = {7},
   url = {https://doi.org/10.3389/fpsyg.2016.01832 https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01832/pdf},
   year = {2016},
}
@article{VanAert2016,
   author = {R C M van Aert and J M Wicherts and M A L M van Assen},
   doi = {10.1177/1745691616650874},
   issue = {5},
   journal = {Perspectives on Psychological Science},
   pages = {713-729},
   title = {Conducting meta-analyses on p-values: Reservations and recommendations for applying p-uniform and p-curve},
   volume = {11},
   url = {https://doi.org/10.1177/1745691616650874 http://journals.sagepub.com/doi/pdf/10.1177/1745691616650874},
   year = {2016},
}
@article{VanAert2018a,
   abstract = {A wide variety of estimators of the between-study variance are available in random-effects meta-analysis. Many, but not all, of these estimators are based on the method of moments. The DerSimonian-Laird estimator is widely used in applications, but the Paule-Mandel estimator is an alternative that is now recommended. Recently, DerSimonian and Kacker have developed two-step moment-based estimators of the between-study variance. We extend these two-step estimators so that multiple (more than two) steps are used. We establish the surprising result that the multistep estimator tends towards the Paule-Mandel estimator as the number of steps becomes large. Hence, the iterative scheme underlying our new multistep estimator provides a hitherto unknown relationship between two-step estimators and Paule-Mandel estimator. Our analysis suggests that two-step estimators are not necessarily distinct estimators in their own right; instead, they are quantities that are closely related to the usual iterative scheme that is used to calculate the Paule-Mandel estimate. The relationship that we establish between the multistep and Paule-Mandel estimator is another justification for the use of the latter estimator. Two-step and multistep estimators are perhaps best conceptualized as approximate Paule-Mandel estimators.},
   author = {R C M van Aert and D Jackson},
   doi = {10.1002/sim.7665},
   isbn = {0277-6715},
   issue = {17},
   journal = {Statistics in Medicine},
   pages = {2616-2629},
   title = {Multistep estimators of the between-study variance: The relationship with the Paule-Mandel estimator},
   volume = {37},
   url = {https://doi.org/10.1002/sim.7665 https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.7665 https://osf.io/dey9x/},
   year = {2018},
}
@article{Klein2018,
   author = {R A Klein and M Vianello and F Hasselman and B G Adams and R B Adams and S Alper and M Aveyard and J R Axt and M T Babalola and S Bahník and R Batra and M Berkics and M J Bernstein and D R Berry and O Bialobrzeska and E D Binan and K Bocian and M J Brandt and R Busching and A C Rédei and H Cai and F Cambier and K Cantarero and C L Carmichael and F Ceric and J Chandler and J-H. Chang and A Chatard and E E Chen and W Cheong and D C Cicero and S Coen and J A Coleman and B Collisson and M A Conway and K S Corker and P G Curran and F Cushman and Z K Dagona and I Dalgar and A Dalla Rosa and W E Davis and M de Bruijn and L De Schutter and T Devos and M de Vries and C Dogulu and N Dozo and K N Dukes and Y Dunham and K Durrheim and C R Ebersole and J E Edlund and A Eller and A S English and C Finck and N Frankowska and M-Á. Freyre and M Friedman and E M Galliani and J C Gandi and T Ghoshal and S R Giessner and T Gill and T Gnambs and Á Gómez and R González and J Graham and J E Grahe and I Grahek and E G T Green and K Hai and M Haigh and E L Haines and M P Hall and M E Heffernan and J A Hicks and P Houdek and J R Huntsinger and H P Huynh and H Ijzerman and Y Inbar and Å H Innes-Ker and W Jiménez-Leal and M-S. John and J A Joy-Gaba and R G Kamiloglu and H B Kappes and S Karabati and H Karick and V N Keller and A Kende and N Kervyn and G Knezevic and C Kovacs and L E Krueger and G Kurapov and J Kurtz and D Lakens and L B Lazarevic and C A Levitan and N A Lewis and S Lins and N P Lipsey and J E Losee and E Maassen and A T Maitner and W Malingumu and R K Mallett and S A Marotta and J Mededovic and F Mena-Pacheco and T L Milfont and W L Morris and S C Murphy and A Myachykov and N Neave and K Neijenhuijs and A J Nelson and F Neto and A Lee Nichols and A Ocampo and S L Odonnell and H Oikawa and M Oikawa and E Ong and G Orosz and M Osowiecka and G Packard and R Pérez-Sánchez and B Petrovic and R Pilati and B Pinter and L Podesta and G Pogge and M M H Pollmann and A M Rutchick and P Saavedra and A K Saeri and E Salomon and K Schmidt and F D Schönbrodt and M B Sekerdej and D Sirlopú and J L M Skorinko and M A Smith and V Smith-Castro and K C H J Smolders and A Sobkow and W Sowden and P Spachtholz and M Srivastava and T G Steiner and J Stouten and C N H Street and O K Sundfelt and S Szeto and E Szumowska and A C W Tang and N Tanzer and M J Tear and J Theriault and M Thomae and D Torres and J Traczyk and J M Tybur and A Ujhelyi and R C M van Aert and M A L M van Assen and M van der Hulst and P A M van Lange and A E van t Veer and A Vásquez- Echeverría and L Ann Vaughn and A Vázquez and L D Vega and C Verniers and M Verschoor and I P J Voermans and M A Vranka and C Welch and A L Wichman and L A Williams and M Wood and J A Woodzicka and M K Wronska and L Young and J M Zelenski and Z Zhijia and B A Nosek},
   doi = {10.1177/2515245918810225},
   isbn = {2515-2459},
   issue = {4},
   journal = {Advances in Methods and Practices in Psychological Science},
   pages = {443-490},
   title = {Many Labs 2: Investigating variation in replicability across samples and settings},
   volume = {1},
   url = {https://doi.org/10.1177/2515245918810225 https://journals.sagepub.com/doi/pdf/10.1177/2515245918810225 https://osf.io/8cd4r/wiki/home/},
   year = {2018},
}
@article{Nuijten2014,
   author = {M B Nuijten and M A L M van Assen and R C M van Aert and J M Wicherts},
   doi = {10.1073/pnas.1322149111},
   isbn = {0027-8424},
   issue = {7},
   journal = {Proc Natl Acad Sci USA},
   pages = {712-713},
   title = {Standard analyses fail to show that US studies overestimate effect sizes in softer research},
   volume = {111},
   url = {https://doi.org/10.1073/pnas.1322149111 NA},
   year = {2014},
}
@article{VanAert2017,
   abstract = {The vast majority of published results in the literature is statistically significant, which raises concerns about their reliability. The Reproducibility Project Psychology (RPP) and Experimental Economics Replication Project (EE-RP) both replicated a large number of published studies in psychology and economics. The original study and replication were statistically significant in 36.1% in RPP and 68.8% in EE-RP suggesting many null effects among the replicated studies. However, evidence in favor of the null hypothesis cannot be examined with null hypothesis significance testing. We developed a Bayesian meta-analysis method called snapshot hybrid that is easy to use and understand and quantifies the amount of evidence in favor of a zero, small, medium and large effect. The method computes posterior model probabilities for a zero, small, medium, and large effect and adjusts for publication bias by taking into account that the original study is statistically significant. We first analytically approximate the methods performance, and demonstrate the necessity to control for the original study’s significance to enable the accumulation of evidence for a true zero effect. Then we applied the method to the data of RPP and EE-RP, showing that the underlying effect sizes of the included studies in EE-RP are generally larger than in RPP, but that the sample sizes of especially the included studies in RPP are often too small to draw definite conclusions about the true effect size. We also illustrate how snapshot hybrid can be used to determine the required sample size of the replication akin to power analysis in null hypothesis significance testing and present an easy to use web application (https://rvanaert.shinyapps.io/snapshot/) and R code for applying the method.},
   author = {R C M van Aert and M A L M van Assen},
   doi = {10.1371/journal.pone.0175302},
   issue = {4},
   journal = {PLOS ONE},
   pages = {e0175302},
   publisher = {Public Library of Science},
   title = {Bayesian evaluation of effect size after replicating an original study},
   volume = {12},
   url = {http://dx.doi.org/10.1371%2Fjournal.pone.0175302 https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0175302&type=printable https://osf.io/fj6bw/},
   year = {2017},
}
@article{OpenScienceCollaboration2015,
   abstract = {Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47% of original effect sizes were in the 95% confidence interval of the replication effect size; 39% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.},
   author = {Open Science Collaboration},
   doi = {10.1126/science.aac4716},
   issue = {6251},
   journal = {Science},
   title = {Estimating the reproducibility of psychological science},
   volume = {349},
   url = {https://doi.org/10.1126/science.aac4716 NA},
   year = {2015},
}
@article{VanAert2018b,
   abstract = {The unrealistically high rate of positive results within psychology has increased the attention to replication research. However, researchers who conduct a replication and want to statistically combine the results of their replication with a statistically significant original study encounter problems when using traditional meta-analysis techniques. The original study’s effect size is most probably overestimated because it is statistically significant, and this bias is not taken into consideration in traditional meta-analysis. We have developed a hybrid method that does take the statistical significance of an original study into account and enables (a) accurate effect size estimation, (b) estimation of a confidence interval, and (c) testing of the null hypothesis of no effect. We analytically approximate the performance of the hybrid method and describe its statistical properties. By applying the hybrid method to data from the Reproducibility Project: Psychology (Open Science Collaboration, 2015), we demonstrate that the conclusions based on the hybrid method are often in line with those of the replication, suggesting that many published psychological studies have smaller effect sizes than those reported in the original study, and that some effects may even be absent. We offer hands-on guidelines for how to statistically combine an original study and replication, and have developed a Web-based application ( https://rvanaert.shinyapps.io/hybrid ) for applying the hybrid method.},
   author = {R C M van Aert and M A L M van Assen},
   doi = {10.3758/s13428-017-0967-6},
   isbn = {1554-3528},
   issue = {4},
   journal = {Behavior Research Methods},
   pages = {1515-1539},
   title = {Examining reproducibility in psychology: A hybrid method for combining a statistically significant original study and a replication},
   volume = {50},
   url = {https://doi.org/10.3758/s13428-017-0967-6 https://link.springer.com/content/pdf/10.3758/s13428-017-0967-6.pdf https://osf.io/6m5b7/},
   year = {2018},
}
@article{Anderson2016a,
   abstract = {Gilbert et al. conclude that evidence from the Open Science Collaboration’s Reproducibility Project: Psychology indicates high reproducibility, given the study methodology. Their very optimistic assessment is limited by statistical misconceptions and by causal inferences from selectively interpreted, correlational data. Using the Reproducibility Project: Psychology data, both optimistic and pessimistic conclusions about reproducibility are possible, and neither are yet warranted.},
   author = {C J Anderson and S Bahnik and M Barnett-Cowan and F A Bosco and J Chandler and C R Chartier and F Cheung and C D Christopherson and A Cordes and E J Cremata and N Della Penna and V Estel and A Fedor and S A Fitneva and M C Frank and J A Grange and J K Hartshorne and F Hasselman and F Henninger and M van der Hulst and K J Jonas and C K Lai and C A Levitan and J K Miller and K S Moore and J M Meixner and M R Munafò and K I Neijenhuijs and G Nilsonne and B A Nosek and F Plessow and J M Prenoveau and A A Ricker and K Schmidt and J R Spies and S Stieger and N Strohminger and G B Sullivan and R C M van Aert and M A L M van Assen and W Vanpaemel and M Vianello and M Voracek and K Zuni},
   doi = {10.1126/science.aad9163},
   issue = {6277},
   journal = {Science},
   pages = {1037},
   title = {Response to comment on 'Estimating the reproducibility of psychological science'},
   volume = {351},
   url = {https://doi.org/10.1126/science.aad9163 https://www.science.org/doi/epdf/10.1126/science.aad9163},
   year = {2016},
}
@article{Augusteijn2019,
   abstract = {One of the main goals of meta-analysis is to test for and estimate the heterogeneity of effect sizes. We examined the effect of publication bias on the Q test and assessments of heterogeneity as a function of true heterogeneity, publication bias, true effect size, number of studies, and variation of sample sizes. The present study has two main contributions and is relevant to all researchers conducting meta-analysis. First, we show when and how publication bias affects the assessment of heterogeneity. The expected values of heterogeneity measures H² and I² were analytically derived, and the power and Type I error rate of the Q test were examined in a Monte Carlo simulation study. Our results show that the effect of publication bias on the Q test and assessment of heterogeneity is large, complex, and nonlinear. Publication bias can both dramatically decrease and increase heterogeneity in true effect size, particularly if the number of studies is large and population effect size is small. We therefore conclude that the Q test of homogeneity and heterogeneity measures H² and I² are generally not valid when publication bias is present. Our second contribution is that we introduce a web application, Q-sense, which can be used to determine the impact of publication bias on the assessment of heterogeneity within a certain meta-analysis and to assess the robustness of the meta-analytic estimate to publication bias. Furthermore, we apply Q-sense to 2 published meta-analyses, showing how publication bias can result in invalid estimates of effect size and heterogeneity. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
   author = {H E M Augusteijn and R C M van Aert and M A L M van Assen},
   city = {US},
   doi = {10.1037/met0000197},
   isbn = {1939-1463(Electronic),1082-989X(Print)},
   issue = {1},
   journal = {Psychological Methods},
   keywords = {*Biased Sampling,*Nonparametric Statistical Tests,*Scientific Communication,*Test Validity,Effect Size (Statistical),Meta Analysis,Sample Size},
   pages = {116-134},
   publisher = {American Psychological Association},
   title = {The effect of publication bias on the Q test and assessment of heterogeneity},
   volume = {24},
   url = {https://doi.org/10.1037/met0000197 https://osf.io/gv25c/download https://osf.io/qzt5z/},
   year = {2019},
}
@article{VanAert2019,
   abstract = {The effect sizes of studies included in a meta-analysis do often not share a common true effect size due to differences in for instance the design of the studies. Estimates of this so-called between-study variance are usually imprecise. Hence, reporting a confidence interval together with a point estimate of the amount of between-study variance facilitates interpretation of the meta-analytic results. Two methods that are recommended to be used for creating such a confidence interval are the Q-profile and generalized Q-statistic method that both make use of the Q-statistic. These methods are exact if the assumptions underlying the random-effects model hold, but these assumptions are usually violated in practice such that confidence intervals of the methods are approximate rather than exact confidence intervals. We illustrate by means of two Monte-Carlo simulation studies with odds ratio as effect size measure that coverage probabilities of both methods can be substantially below the nominal coverage rate in situations that are representative for meta-analyses in practice. We also show that these too low coverage probabilities are caused by violations of the assumptions of the random-effects model (ie, normal sampling distributions of the effect size measure and known sampling variances) and are especially prevalent if the sample sizes in the primary studies are small.},
   author = {R C M van Aert and M A L M van Assen and W Viechtbauer},
   doi = {10.1002/jrsm.1336},
   isbn = {1759-2879},
   issue = {2},
   journal = {Research Synthesis Methods},
   pages = {225-239},
   title = {Statistical properties of methods based on the Q-statistic for constructing a confidence interval for the between-study variance in meta-analysis},
   volume = {10},
   url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jrsm.1336 https://onlinelibrary.wiley.com/doi/epdf/10.1002/jrsm.1336/ https://osf.io/kcaqb/},
   year = {2019},
}
@article{VanAssen2014,
   abstract = {BACKGROUND: De Winter and Happee examined whether science based on selective publishing of significant results may be effective in accurate estimation of population effects, and whether this is even more effective than a science in which all results are published (i.e., a science without publication bias). Based on their simulation study they concluded that "selective publishing yields a more accurate meta-analytic estimation of the true effect than publishing everything, (and that) publishing nonreplicable results while placing null results in the file drawer can be beneficial for the scientific collective" (p.4). METHODS AND FINDINGS: Using their scenario with a small to medium population effect size, we show that publishing everything is more effective for the scientific collective than selective publishing of significant results. Additionally, we examined a scenario with a null effect, which provides a more dramatic illustration of the superiority of publishing everything over selective publishing. CONCLUSION: Publishing everything is more effective than only reporting significant outcomes.},
   author = {M A L M van Assen and R C M van Aert and M B Nuijten and J M Wicherts},
   doi = {10.1371/journal.pone.0084896},
   issue = {1},
   journal = {PLOS ONE},
   pages = {e84896},
   title = {Why publishing everything is more effective than selective publishing of statistically significant results},
   volume = {9},
   url = {https://doi.org/10.1371/journal.pone.0084896 http://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0084896&type=printable},
   year = {2014},
}
@article{VanAert2019d,
   abstract = {Publication bias is a substantial problem for the credibility of research in general and of meta-analyses in particular, as it yields overestimated effects and may suggest the existence of non-existing effects. Although there is consensus that publication bias exists, how strongly it affects different scientific literatures is currently less well-known. We examined evidence of publication bias in a large-scale data set of primary studies that were included in 83 meta-analyses published in Psychological Bulletin (representing meta-analyses from psychology) and 499 systematic reviews from the Cochrane Database of Systematic Reviews (CDSR; representing meta-analyses from medicine). Publication bias was assessed on all homogeneous subsets (3.8% of all subsets of meta-analyses published in Psychological Bulletin) of primary studies included in meta-analyses, because publication bias methods do not have good statistical properties if the true effect size is heterogeneous. Publication bias tests did not reveal evidence for bias in the homogeneous subsets. Overestimation was minimal but statistically significant, providing evidence of publication bias that appeared to be similar in both fields. However, a Monte-Carlo simulation study revealed that the creation of homogeneous subsets resulted in challenging conditions for publication bias methods since the number of effect sizes in a subset was rather small (median number of effect sizes equaled 6). Our findings are in line with, in its most extreme case, publication bias ranging from no bias until only 5% statistically nonsignificant effect sizes being published. These and other findings, in combination with the small percentages of statistically significant primary effect sizes (28.9% and 18.9% for subsets published in Psychological Bulletin and CDSR), led to the conclusion that evidence for publication bias in the studied homogeneous subsets is weak, but suggestive of mild publication bias in both psychology and medicine.},
   author = {R C M van Aert and J M Wicherts and M A L M van Assen},
   doi = {10.1371/journal.pone.0215052},
   issue = {4},
   journal = {PLOS ONE},
   pages = {e0215052},
   title = {Publication bias examined in meta-analyses from psychology and medicine: A meta-meta-analysis},
   volume = {14},
   url = {https://doi.org/10.1371/journal.pone.0215052 https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0215052&type=printable https://osf.io/9jqht/},
   year = {2019},
}
@article{Niemeyer2020,
   author = {H Niemeyer and R C M van Aert and S Schmid and D Uelsmann and C Knaevelsrud and O Schulte-Herbrueggen},
   doi = {10.15626/MP.2018.884},
   journal = {Meta-Psychology},
   title = {Publication bias in meta-analyses of posttraumatic stress disorder interventions},
   volume = {4},
   url = {https://doi.org/10.15626/MP.2018.884 https://open.lnu.se/index.php/metapsychology/article/view/884/2308 https://osf.io/x7897/},
   year = {2020},
}
@article{VanAert2019g,
   abstract = {The Hartung-Knapp method for random-effects meta-analysis, that was also independently proposed by Sidik and Jonkman, is becoming advocated for general use. This method has previously been justified by taking all estimated variances as known and using a different pivotal quantity to the more conventional one when making inferences about the average effect. We provide a new conceptual framework for, and justification of, the Hartung-Knapp method. Specifically, we show that inferences from fitted random-effects models, using both the conventional and the Hartung-Knapp method, are equivalent to those from closely related intercept only weighted least squares regression models. This observation provides a new link between Hartung and Knapp's methodology for meta-analysis and standard linear models, where it can be seen that the Hartung-Knapp method can be justified by a linear model that makes a slightly weaker assumption than taking all variances as known. This provides intuition for why the Hartung-Knapp method has been found to perform better than the conventional one in simulation studies. Furthermore, our new findings give more credence to ad hoc adjustments of confidence intervals from the Hartung-Knapp method that ensure these are at least as wide as more conventional confidence intervals. The conceptual basis for the Hartung-Knapp method that we present here should replace the established one because it more clearly illustrates the potential benefit of using it.},
   author = {R C M van Aert and D Jackson},
   doi = {10.1002/jrsm.1356},
   isbn = {1759-2879},
   issue = {4},
   journal = {Research Synthesis Methods},
   pages = {515-527},
   title = {A new justification of the Hartung-Knapp method for random-effects meta-analysis based on weighted least squares regression},
   volume = {10},
   url = {https://doi.org/10.1002/jrsm.1356 https://onlinelibrary.wiley.com/doi/epdf/10.1002/jrsm.1356 https://osf.io/k7n3s/},
   year = {2019},
}
@article{VanderMeulen2020,
   abstract = {Background Military personnel are exposed to severe stressors across different stages of their career that may have a negative impact on mental health and functioning. It is often suggested that psychological resilience plays an important role in the maintenance and/or enhancement of their mental health and functioning under these circumstances. Method A systematic literature search was conducted using PsycINFO, MEDLINE, PsycARTICLES, Psychology and Behavioral Sciences Collection, Web of Science, and PubMed up to August of 2019 retrieving 3,698 reports. Schmidt and Hunter meta-analytical techniques were used to assess the predictive value of psychological resilience on ten different military relevant mental health and functioning outcomes. Multivariate meta-analysis assessed the origin of heterogeneity among bivariate effect sizes. Results The effect sizes of 40 eligible peer-reviewed papers covering 40 unique samples were included in the meta-analysis. Seventy-eight percent of these studies were published after 2010 and were predominantly conducted in western countries. Bivariate effect sizes were low to medium (absolute values: 0.08 to 0.36) and multivariate effect sizes, adjusting for across studies varying sets of covariates, were low to trivial (absolute values: 0.02 to 0.08). Moderator analyses using multivariate meta-analysis on 60 bivariate effect sizes, revealed no significant effect of type of psychological resilience scale, time-lag, and career stage. Conclusions The current review found no indications that different conceptualizations of psychological resilience across a variety of research designs, are strongly predictive of mental health and functioning among military personnel. Future directions (moderator/mediator models, stressor type specifications, and directionality) for prospective studies are discussed. Our results question the usefulness of interventions to enhance the resilience of soldiers to improve their mental health and functioning.},
   author = {E van der Meulen and P G van der Velden and R C M van Aert and M J P M van Veldhoven},
   doi = {10.1016/j.socscimed.2020.112814},
   isbn = {0277-9536},
   journal = {Social Science & Medicine},
   keywords = {Meta-analysis,Military,Prospective,Psychological resilience,Review},
   pages = {112814},
   title = {Longitudinal associations of psychological resilience with mental health and functioning among military personnel: A meta-analysis of prospective studies},
   volume = {255},
   url = {https://www.sciencedirect.com/science/article/abs/pii/S0277953620300332?via%3Dihub https://www.sciencedirect.com/science/article/abs/pii/S0277953620300332?via%3Dihub},
   year = {2020},
}
@article{Havranek2020,
   abstract = {Abstract Meta-analysis has become the conventional approach to synthesizing the results of empirical economics research. To further improve the transparency and replicability of the reported results and to raise the quality of meta-analyses, the Meta-Analysis of Economics Research Network has updated the reporting guidelines that were published by this Journal in 2013. Future meta-analyses in economics will be expected to follow these updated guidelines or give valid reasons why a meta-analysis should deviate from them.},
   author = {T Havranek and T D Stanley and H Doucouliagos and P Bom and J Geyer-Klingeberg and I Iwasaki and W R Reed and K Rost and R C M van Aert},
   doi = {10.1111/joes.12363},
   isbn = {0950-0804},
   issue = {3},
   journal = {Journal of Economic Surveys},
   pages = {469-475},
   title = {Reporting guidelines for meta-analysis in economics},
   volume = {34},
   url = {https://doi.org/10.1111/joes.12363 https://onlinelibrary.wiley.com/doi/epdf/10.1111/joes.12363},
   year = {2020},
}
@article{VanderArk2015,
   abstract = {This study was motivated by the question which type of confidence interval (CI) one should use to summarize sample variance of Goodman and Kruskal's coefficient gamma. In a Monte-Carlo study, we investigated the coverage and computation time of the Goodman?Kruskal CI, the Cliff-consistent CI, the profile likelihood CI, and the score CI for Goodman and Kruskal's gamma, under several conditions. The choice for Goodman and Kruskal's gamma was based on results of Woods [Consistent small-sample variances for six gamma-family measures of ordinal association. Multivar Behav Res. 2009;44:525?551], who found relatively poor coverage for gamma for very small samples compared to other ordinal association measures. The profile likelihood CI and the score CI had the best coverage, close to the nominal value, but those CIs could often not be computed for sparse tables. The coverage of the Goodman?Kruskal CI and the Cliff-consistent CI was often poor. Computation time was fast to reasonably fast for all types of CI.},
   author = {L A van der Ark and R C M van Aert},
   doi = {10.1080/00949655.2014.932791},
   isbn = {0094-9655},
   issue = {12},
   journal = {Journal of Statistical Computation and Simulation},
   pages = {2491-2505},
   publisher = {Taylor & Francis},
   title = {Comparing confidence intervals for Goodman and Kruskal's gamma coefficient},
   volume = {85},
   url = {https://doi.org/10.1080/00949655.2014.932791 NA},
   year = {2015},
}
@article{VanAssen2020,
   author = {M A L M van Assen and O R van den Akker and H E M Augusteijn and M Bakker and M B Nuijten and A Olsson-Collentine and A H Stoevenbelt and J M Wicherts and R C M van Aert},
   title = {The meta-plot: A graphical tool for interpreting the results of a meta-analysis. Manuscript submitted for publication},
   year = {2021},
}
@article{Ross2020,
   author = {R R Ross and R C M van Aert and O R van den Akker and M van Elk},
   doi = {10.1017/S0140525X20000606},
   issue = {e19},
   journal = {Behavioral and Brain Sciences},
   title = {The role of meta-analysis and preregistration in assessing the evidence for cleansing effects},
   volume = {44},
   url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/role-of-metaanalysis-and-preregistration-in-assessing-the-evidence-for-cleansing-effects/E3D6970360915C34DA221A6616FED9B0 https://pure.uvt.nl/ws/portalfiles/portal/49812532/MTO_v_Aert_the_role_of_meta_analysis_and_preregistration_BaBS_2021_Postprint.pdf},
   year = {2021},
}
@article{VanAert2020,
   author = {R C M van Aert},
   doi = {10.31222/osf.io/9tmua},
   journal = {Zeitschrift fur Psychologie},
   title = {Analyzing data of a multi-lab replication project with individual participant data meta-analysis: A tutorial},
   url = {https://osf.io/preprints/metaarxiv/9tmua/ https://osf.io/preprints/metaarxiv/9tmua/download https://osf.io/m7tu9/},
}
@article{VanAert2020a,
   author = {R C M van Aert and C H Schmid and D Svensson and D Jackson},
   doi = {10.1002/jrsm.1490},
   issue = {4},
   journal = {Research Synthesis Methods},
   pages = {429-447},
   title = {Study specific prediction intervals for random-effects meta-analysis},
   volume = {12},
   url = {https://doi.org/10.1002/jrsm.1490 https://onlinelibrary.wiley.com/doi/epdf/10.1002/jrsm.1490 https://osf.io/4uq93/},
   year = {2021},
}
@article{VanAert2021c,
   author = {R C M van Aert and J M Wicherts},
   doi = {10.31222/osf.io/bn8vd},
   title = {Correcting for outcome reporting bias in a meta-analysis: A meta-regression approach. Manuscript submitted for publication},
   url = {https://osf.io/preprints/metaarxiv/bn8vd/ https://osf.io/preprints/metaarxiv/bn8vd/download https://osf.io/wzmh9/},
   year = {2021},
}
@article{VanAert2021b,
   author = {R C M van Aert and M B Nuijten and A Olsson-Collentine and A H Stoevenbelt and O R van den Akker and J M Wicherts},
   journal = {Royal Society Open Science},
   title = {Comparing the prevalence of statistical reporting inconsistencies in COVID-19 preprints and matched controls: A Registered Report. Conditionally accepted},
   url = {NA https://osf.io/2yn8c/download},
   year = {2021},
}
@article{Muldera,
   author = {J Mulder and D R Williams and X Gu and A Tomarken and F Boeing-Messing and A Olsson-Collentine and M Meijerink-Bosman and J Menke and R C M van Aert and J.-P. Fox and H Hoijtink and Y Rosseel and E.-J. Wagenmakers and C van Lissa},
   doi = {10.18637/jss.v100.i18},
   issue = {18},
   journal = {Journal of Statistical Software},
   pages = {1-63},
   title = {BFpack: Flexible Bayes factor testing of scientific theories in R},
   volume = {100},
   url = {https://www.jstatsoft.org/index.php/jss/article/view/v100i18 https://www.jstatsoft.org/index.php/jss/article/view/v100i18/4207},
   year = {2021},
}
@article{Olsson-Collentine2021,
   author = {A Olsson-Collentine and R C M van Aert and M Bakker and J M Wicherts},
   doi = {10.31234/osf.io/43yae},
   title = {Meta-analyzing the multiverse: A peek under the hood of selective reporting. Manuscript submitted for publication},
   url = {https://doi.org/10.31234/osf.io/43yae https://psyarxiv.com/43yae/download https://osf.io/j8yg2/},
   year = {2021},
}
@article{Isager2021,
   author = {P M Isager and R C M van Aert and S Bahnik and M J Brandt and A DeSoto and R Giner-Sorolla and J I Krueger and M Perugini and I Ropovik and A E van 't Veer and M Vranka and D Lakens},
   doi = {10.31222/osf.io/2gurz},
   journal = {Psychological Methods},
   title = {Deciding what to replicate: A formal definition of 'replication value' and a decision model for replication study selection},
   url = {https://doi.org/10.31222/osf.io/2gurz https://osf.io/preprints/metaarxiv/2gurz/download https://osf.io/asype/},
}
@article{Brohmer2021,
   author = {H Brohmer and L V Eckerstorfer and R C M van Aert and K Corcoran},
   doi = {10.5334/irsp.428},
   issue = {1},
   journal = {International Review of Social Psychology},
   pages = {3},
   title = {Do behavioral observations make people catch the goal? A meta-analysis on goal contagion},
   volume = {34},
   url = {https://www.rips-irsp.com/articles/10.5334/irsp.428/ https://www.rips-irsp.com/articles/10.5334/irsp.428/galley/205/download/ https://osf.io/mxepy/},
   year = {2021},
}
@book_section{VanAert2021a,
   author = {R C M van Aert and H Niemeyer},
   doi = {10.31222/osf.io/3rdys},
   editor = {S O Lilienfeld and W O'Donohue and A Masuda},
   journal = {Questionable research practices in clinical psychology},
   publisher = {Springer},
   title = {Publication bias},
   url = {https://doi.org/10.31222/osf.io/3rdys https://osf.io/preprints/metaarxiv/3rdys/download https://osf.io/pfzyx/},
}
@article{VanAert2021,
   author = {R C M van Aert and J Mulder},
   doi = {10.3758/s13423-021-01918-9},
   journal = {Psychonomic Bulletin & Review},
   title = {Bayesian hypothesis testing and estimation under the marginalized random-effects meta-analysis model},
   url = {https://doi.org/10.3758/s13423-021-01918-9 https://link.springer.com/content/pdf/10.3758/s13423-021-01918-9.pdf https://osf.io/z3csy},
   year = {2021},
}
@article{Schaerer2021,
   author = {M Schaerer and M Nguyen and C du Plessis and R C M van Aert and L Tiokhin and D Lakens and E L Uhlmann},
   title = {Meta-analysis of 44 years of field audits reveals a decline in gender discrimination across time. Manuscript submitted for publication},
   year = {2021},
}
@article{Schweinsberg2021,
   abstract = {In this crowdsourced initiative, independent analysts used the same dataset to test two hypotheses regarding the effects of scientists’ gender and professional status on verbosity during group meetings. Not only the analytic approach but also the operationalizations of key variables were left unconstrained and up to individual analysts. For instance, analysts could choose to operationalize status as job title, institutional ranking, citation counts, or some combination. To maximize transparency regarding the process by which analytic choices are made, the analysts used a platform we developed called DataExplained to justify both preferred and rejected analytic paths in real time. Analyses lacking sufficient detail, reproducible code, or with statistical errors were excluded, resulting in 29 analyses in the final sample. Researchers reported radically different analyses and dispersed empirical outcomes, in a number of cases obtaining significant effects in opposite directions for the same research question. A Boba multiverse analysis demonstrates that decisions about how to operationalize variables explain variability in outcomes above and beyond statistical choices (e.g., covariates). Subjective researcher decisions play a critical role in driving the reported empirical results, underscoring the need for open data, systematic robustness checks, and transparency regarding both analytic paths taken and not taken. Implications for organizations and leaders, whose decision making relies in part on scientific findings, consulting reports, and internal analyses by data scientists, are discussed.},
   author = {M Schweinsberg and M Feldman and N Staub and O R van den Akker and R C M van Aert and M A L M van Assen and Y Liu and T Althoff and J Heer and A Kale and Z Mohamed and H Amireh and V Venkatesh Prasad and A Bernstein and E Robinson and K Snellman and S Amy Sommer and S M G Otner and D Robinson and N Madan and R Silberzahn and P Goldstein and W Tierney and T Murase and B Mandl and D Viganola and C Strobl and C B C Schaumans and S Kelchtermans and C Naseeb and S Mason Garrison and T Yarkoni and C S Richard Chan and P Adie and P Alaburda and C Albers and S Alspaugh and J Alstott and A A Nelson and E Ariño de la Rubia and A Arzi and Š Bahník and J Baik and L Winther Balling and S Banker and D Aa Baranger and D J Barr and B Barros-Rivera and M Bauer and E Blaise and L Boelen and K Bohle Carbonell and R A Briers and O Burkhard and M.-A. Canela and L Castrillo and T Catlett and O Chen and M Clark and B Cohn and A Coppock and N Cugueró-Escofet and Paul G Curran and W Cyrus-Lai and D Dai and G Valentino Dalla Riva and H Danielsson and R de F S M Russo and N de Silva and C Derungs and F Dondelinger and C Duarte de Souza and B Tyson Dube and M Dubova and B Mark Dunn and P Adriaan Edelsbrunner and S Finley and N Fox and T Gnambs and Y Gong and E Grand and B Greenawalt and D Han and P H P Hanel and A B Hong and D Hood and J Hsueh and L Huang and K N Hui and K A Hultman and A Javaid and L Ji Jiang and J Jong and J Kamdar and D Kane and G Kappler and E Kaszubowski and C M Kavanagh and M Khabsa and B Kleinberg and J Kouros and H Krause and A.-M. Krypotos and D Lavbič and R Ling Lee and T Leffel and W Yang Lim and S Liverani and B Loh and D Lønsmann and J Wei Low and A Lu and K MacDonald and C R Madan and L Hjorth Madsen and C Maimone and A Mangold and A Marshall and H Ester Matskewich and K Mavon and K L McLain and A A McNamara and M McNeill and U Mertens and D Miller and B Moore and A Moore and E Nantz and Z Nasrullah and V Nejkovic and C S Nell and A Arthur Nelson and G Nilsonne and R Nolan and C E O'Brien and P O'Neill and K O'Shea and T Olita and J Otterbacher and D Palsetia and B Pereira and I Pozdniakov and J Protzko and J.-N. Reyt and T Riddle and A Ridhwan Omar Ali and I Ropovik and J M Rosenberg and S Rothen and M Schulte-Mecklenbeck and N Sharma and G Shotwell and M Skarzynski and W Stedden and V Stodden and M A Stoffel and S Stoltzman and S Subbaiah and R Tatman and P H Thibodeau and S Tomkins and A Valdivia and G B Druijff-van de Woestijne and L Viana and F Villesèche and W Duncan Wadsworth and F Wanders and K Watts and J D Wells and C E Whelpley and A Won and L Wu and A Yip and C Youngflesh and J.-C. Yu and A Zandian and L Zhang and C Zibman and E L Uhlmann},
   doi = {10.1016/j.obhdp.2021.02.003},
   issn = {0749-5978},
   journal = {Organizational Behavior and Human Decision Processes},
   keywords = {Analysis-contingent results,Crowdsourcing data analysis,Research reliability,Researcher degrees of freedom,Scientific robustness,Scientific transparency},
   note = {(Akmal)},
   pages = {228-249},
   title = {Same data, different conclusions: Radical dispersion in empirical results when independent analysts operationalize and test the same hypothesis},
   volume = {165},
   url = {https://www.sciencedirect.com/science/article/pii/S0749597821000200 https://www.sciencedirect.com/science/article/pii/S0749597821000200},
   year = {2021},
}
